{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47958\n",
      "{'nat', 'geo', 'art', 'gpe', 'eve', 'tim', 'per', 'O', 'org'}\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "import pandas\n",
    "import numpy as np\n",
    "import gensim\n",
    "import csv\n",
    "from keras.preprocessing import sequence\n",
    "dataset=[]\n",
    "sent=[]\n",
    "ne_tags=set()\n",
    "with open('ner_dataset.csv',newline='\\n') as csvfile:\n",
    "    lines = csv.reader(csvfile,delimiter=',',quotechar='\"')\n",
    "    for item in lines:\n",
    "        ne_tags.add(item[-1].split('-')[-1])\n",
    "        if item[0]:\n",
    "            if len(sent):\n",
    "                dataset.append(sent)\n",
    "            sent=[item[1:]]\n",
    "        else:\n",
    "            sent.append(item[1:])\n",
    "print(len(dataset))\n",
    "print(ne_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592 38366\n"
     ]
    }
   ],
   "source": [
    "max_len=max([len(sent) for sent in dataset])\n",
    "dataset_size = len(dataset)\n",
    "indices = np.random.permutation(dataset_size)\n",
    "train_ind, test_ind = indices[:int(dataset_size*0.8)], indices[int(dataset_size*0.8):]\n",
    "train_set,test_set=[],[]\n",
    "for ind in train_ind:\n",
    "    train_set.append(dataset[ind])\n",
    "for ind in test_ind:\n",
    "    test_set.append(dataset[ind])\n",
    "train_sents=[[t[0] for t in sent] for sent in train_set]\n",
    "train_pos=[[t[1] for t in sent] for sent in train_set]\n",
    "train_ne=[[t[2].split('-')[-1] for t in sent] for sent in train_set]\n",
    "test_sents=[[t[0] for t in sent] for sent in test_set]\n",
    "test_pos=[[t[1] for t in sent] for sent in test_set]\n",
    "test_ne=[[t[2].split('-')[-1] for t in sent] for sent in test_set]\n",
    "print(len(test_set),len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38366, 104, 100) (38366, 104, 9)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(labels,label='O'):\n",
    "    vec=[0]*len(labels)\n",
    "    vec[list(labels).index(label)]=1\n",
    "    return np.array(vec)\n",
    "lmodel=gensim.models.Word2Vec(train_sents,size=100)\n",
    "train_x,train_y=[],[]\n",
    "for i,sent in enumerate(train_sents):\n",
    "    sent_len=len(sent)\n",
    "    ele_vec=[np.zeros(100)]*(max_len-sent_len)\n",
    "#     print(np.array(ele_vec).shape)\n",
    "    ele_ne=[one_hot(ne_tags)]*(max_len-sent_len)\n",
    "    for j,word in enumerate(sent):\n",
    "        try:\n",
    "            ele_vec.append(lmodel.wv[word])\n",
    "            ele_ne.append(one_hot(ne_tags,train_ne[i][j]))\n",
    "        except:\n",
    "            new_wv = 2*np.random.randn(100)-1 # sample from normal distn\n",
    "            norm_const = np.linalg.norm(new_wv)\n",
    "            new_wv /= norm_const\n",
    "            ele_vec.append(new_wv)\n",
    "            ele_ne.append(one_hot(ne_tags))\n",
    "    train_x.append(ele_vec)\n",
    "    train_y.append(ele_ne)\n",
    "train_x=np.array(train_x)\n",
    "train_y=np.array(train_y)\n",
    "print(train_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9592, 104, 100) (9592, 104, 9)\n"
     ]
    }
   ],
   "source": [
    "test_x,test_y=[],[]\n",
    "for i,sent in enumerate(test_sents):\n",
    "    sent_len=len(sent)\n",
    "    ele_vec=[np.zeros(100)]*(max_len-sent_len)\n",
    "#     print(np.array(ele_vec).shape)\n",
    "    ele_ne=[one_hot(ne_tags)]*(max_len-sent_len)\n",
    "    for j,word in enumerate(sent):\n",
    "        try:\n",
    "            ele_vec.append(lmodel.wv[word])\n",
    "            ele_ne.append(one_hot(ne_tags,test_ne[i][j]))\n",
    "        except:\n",
    "            new_wv = 2*np.random.randn(100)-1 # sample from normal distn\n",
    "            norm_const = np.linalg.norm(new_wv)\n",
    "            new_wv /= norm_const\n",
    "            ele_vec.append(new_wv)\n",
    "            ele_ne.append(one_hot(ne_tags))\n",
    "    test_x.append(ele_vec)\n",
    "    test_y.append(ele_ne)\n",
    "test_x=np.array(test_x)\n",
    "test_y=np.array(test_y)\n",
    "print(test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed,Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_model=Sequential()\n",
    "ner_model.add(Bidirectional(LSTM(150,return_sequences=True,\n",
    "                                 bias_regularizer=l2(0),\n",
    "                                 activity_regularizer=l2(0.),\n",
    "                                 kernel_regularizer=l2(0.))\n",
    "                            ,\n",
    "                            input_shape=(104,100)\n",
    "                           ))\n",
    "ner_model.add(Dropout(0.5))\n",
    "ner_model.add(TimeDistributed(Dense(9,activation='softmax',\n",
    "                                    bias_regularizer=l2(0.),\n",
    "                                   activity_regularizer=l2(0.),\n",
    "                                   kernel_regularizer=l2(0.))))\n",
    "ner_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "38366/38366 [==============================] - 412s - loss: 0.1862 - acc: 0.9716   \n",
      "Epoch 2/5\n",
      "38366/38366 [==============================] - 436s - loss: 0.0520 - acc: 0.9850   \n",
      "Epoch 3/5\n",
      "38366/38366 [==============================] - 448s - loss: 0.0430 - acc: 0.9873   \n",
      "Epoch 4/5\n",
      "38366/38366 [==============================] - 464s - loss: 0.0386 - acc: 0.9886   \n",
      "Epoch 5/5\n",
      "38366/38366 [==============================] - 465s - loss: 0.0357 - acc: 0.9894   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x104cd1a58>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.fit(train_x,train_y,epochs=5,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.06%\n"
     ]
    }
   ],
   "source": [
    "scores = ner_model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('International', 'org'), ('Atomic', 'org'), ('Energy', 'org'), ('Agency', 'org'), ('is', 'O'), ('a', 'O'), ('place', 'O'), ('in', 'O'), ('America', 'geo'), (',', 'O'), ('while', 'O'), ('World', 'O'), ('Trade', 'org'), ('Organization', 'org'), ('is', 'O'), ('a', 'O'), ('place', 'O'), ('in', 'O'), ('Canada', 'geo')]\n"
     ]
    }
   ],
   "source": [
    "sentence='International Atomic Energy Agency is a place in America , while World Trade Organization is a place in Canada'\n",
    "sentence=sentence.split()\n",
    "sent_len=len(sentence)\n",
    "ele_vec=[np.zeros(100)]*(max_len-sent_len)\n",
    "for word in sentence:\n",
    "    try:\n",
    "        ele_vec.append(lmodel.wv[word])\n",
    "    except:\n",
    "        new_wv = 2*np.random.randn(100)-1 # sample from normal distn\n",
    "        norm_const = np.linalg.norm(new_wv)\n",
    "        new_wv /= norm_const\n",
    "        ele_vec.append(new_wv)\n",
    "X=np.array(ele_vec).reshape((1,max_len,100))\n",
    "score = ner_model.predict(X,batch_size=1)\n",
    "# print(score[0][-sent_len:][:])\n",
    "res=[]\n",
    "for i,s in enumerate(score[0][-sent_len:]):\n",
    "    res.append((sentence[i],list(ne_tags)[np.argmax(s)]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
